{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db5cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import WhitespaceTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff949e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_space_tokenizer = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33168150",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_5_sentences = \"data.txt\"\n",
    "file_name_40_sentences = \"data_40_cau_tieng_Viet.txt\"\n",
    "\n",
    "file_name_dictionary = \"Viet74K.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12daf633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senteces in data: 2\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "\n",
    "with open(file_name_5_sentences, \"r\", encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        sentence = content[i]\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "print(\"Number of senteces in data:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e35b09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in dictionary: 73901\n"
     ]
    }
   ],
   "source": [
    "dictionary_corpus = list()\n",
    "\n",
    "with open(file_name_dictionary, \"r\", encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        term = content[i].strip().lower()\n",
    "        dictionary_corpus.append(term)\n",
    "        \n",
    "dictionary_corpus\n",
    "\n",
    "print(\"Number of terms in dictionary:\", len(dictionary_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1c64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessString(string_data):\n",
    "    # Lower the text\n",
    "    preprocess_data = string_data.lower()\n",
    "    \n",
    "    # Remove punctuations, each punctuation = space, ex: \"\"information @#$retrieval\" -> \"information    retrieval\"\n",
    "    preprocess_data = re.sub('[%s]' % re.escape(string.punctuation), ' ', preprocess_data)   \n",
    "        \n",
    "    # Tokenize word by white space\n",
    "    preprocess_data = white_space_tokenizer.tokenize(preprocess_data)\n",
    "      \n",
    "    return preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4684232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTerm(sentence_tokenized, start_index, end_index):\n",
    "    term = \"\"\n",
    "    for i in range(start_index, end_index):\n",
    "        term += sentence_tokenized[i] + \" \"\n",
    "    \n",
    "    return term.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49476ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_MAX_TERM_VIETNAMESE = 4\n",
    "\n",
    "sentences_tokenized = dict()\n",
    "\n",
    "for ith_sentence, sentence in enumerate(sentences):\n",
    "    \"\"\" Hiện tại thì chỗ này xét cả câu luôn, về sau sẽ nâng cấp lên thành xét các vế trong câu \"\"\"\n",
    "    sentence_part_tokenzied = PreprocessString(sentence)\n",
    "    \n",
    "    ith_start_term = 0\n",
    "    if len(sentence_part_tokenzied) > LEN_MAX_TERM_VIETNAMESE:\n",
    "        ith_end_term = ith_start_term + LEN_MAX_TERM_VIETNAMESE\n",
    "    else:\n",
    "        ith_end_term = len(sentence_part_tokenzied)\n",
    "        \n",
    "    term = GetTerm(sentence_part_tokenzied, ith_start_term, ith_end_term)\n",
    "    \n",
    "    # Xét hết cả vế\n",
    "    while True:\n",
    "        \n",
    "        # Xét hết cả term\n",
    "        while ith_start_term != ith_end_term:\n",
    "            if term in dictionary_corpus:\n",
    "                \n",
    "                if ith_sentence not in sentences_tokenized.keys():\n",
    "                    sentences_tokenized[ith_sentence] = [term]\n",
    "                else:\n",
    "                    sentences_tokenized[ith_sentence].append(term)\n",
    "#                 print(\"Trùng được term:\", term)\n",
    "                    \n",
    "                break\n",
    "            else:\n",
    "#                 print(\"không trùng term nào cả, term:\", term)\n",
    "                ith_end_term -= 1\n",
    "                term = GetTerm(sentence_part_tokenzied, ith_start_term, ith_end_term)\n",
    "\n",
    "        # Đã xét xong term cuối cùng thì chuyển sang câu (vế) khác\n",
    "        if ith_end_term == len(sentence_part_tokenzied):\n",
    "            break\n",
    "        # Nếu vẫn còn term để xét thì chuyển sang term kế tiếp\n",
    "        else:\n",
    "            # Đề phòng trường hợp term không có trong dictionary thì end sẽ tiến về và bằng start, và sẽ gây ra vòng lặp vô tận\n",
    "            # nếu cứ gắn start bằng lại end (giá trị start ban đầu) và cộng end = start + LEN_MAX_TERM_VIETNAMESE\n",
    "            # vd: term = \"2 mũi vaccine cơ\", start = 16, end = 20, chữ \"2\" không có trong dictionary nên sẽ khiến end bị trừ\n",
    "            # thành 16 (bằng với start), và nếu không có điều kiện kiểm tra \"ith_start_term == ith_end_term\" thì sẽ khiến\n",
    "            # start và end trở lại như cũ (16 và 20), vì ở đây gán start = end và đoạn bên dưới gán end = start + LEN_MAX_TERM_VIETNAMESE\n",
    "            if ith_start_term == ith_end_term:\n",
    "                ith_start_term = ith_end_term + 1\n",
    "            else:\n",
    "                ith_start_term = ith_end_term\n",
    "                \n",
    "            # Nếu vẫn còn nhiều hơn 4 term để xét thì chuyển sang 4 term kế tiếp\n",
    "            if len(sentence_part_tokenzied) - ith_end_term > LEN_MAX_TERM_VIETNAMESE:\n",
    "                ith_end_term = ith_start_term + LEN_MAX_TERM_VIETNAMESE\n",
    "            # Nếu chỉ còn nhiều nhất là 4 term thì lấy ra những term cuối cùng của vế để xét\n",
    "            else:\n",
    "                ith_end_term = len(sentence_part_tokenzied)\n",
    "                \n",
    "            term = GetTerm(sentence_part_tokenzied, ith_start_term, ith_end_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d79c6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tôi', 'n', 'chơi', 'v', 'cầu lông', 'a']\n",
      "['tôi', 'n', 'đi', 'v', 'tập', 'v', 'thể dục', 'a']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key in  sentences_tokenized.keys():\n",
    "    print(sentences_tokenized[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2622d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [e for e in sentences_tokenized.values() ]\n",
    "list_words = []\n",
    "list_pos = []\n",
    "for e in l:\n",
    "    pos = [i for i in e if len(i) ==1 ]\n",
    "    words = [i for i in e if len(i) > 1 ]\n",
    "    list_words.append(words)\n",
    "    list_pos.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36e792e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tôi', 'chơi', 'cầu lông'], ['tôi', 'đi', 'tập', 'thể dục']]\n",
      "[['n', 'v', 'a'], ['n', 'v', 'v', 'a']]\n"
     ]
    }
   ],
   "source": [
    "print(list_words)\n",
    "print(list_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79b41b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tìm ra tất cả các pos có trong data  không trùng nhau \n",
    "def findPosAndWords(list_pos,list_words):\n",
    "    tmp1 = []\n",
    "    tmp2 = []\n",
    "    set_pos =set()\n",
    "    set_words = set()\n",
    "    # convert all to set \n",
    "    for i in list_pos:\n",
    "        tmp1.append(set(i))\n",
    "    for i in list_words:\n",
    "        tmp2.append(set(i))\n",
    "    # Union all \n",
    "    for i in tmp1:\n",
    "        set_pos = set.union (set_pos,i)\n",
    "    for i in tmp2:\n",
    "        set_words = set.union (set_words,i)\n",
    "    return list(set_words),list(set_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "918c8e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cầu lông', 'thể dục', 'đi', 'tập', 'tôi', 'chơi'] ['n', 'a', 'v']\n"
     ]
    }
   ],
   "source": [
    "set_words, set_pos = findPosAndWords(list_pos,list_words)\n",
    "print(set_words,set_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0043f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dicA(set_pos):\n",
    "    dicA = dict()\n",
    "    set_pos1  = ['s'] + set_pos \n",
    "    for i in set_pos1:\n",
    "        for j in set_pos:\n",
    "            key = i+j \n",
    "            dicA[key] = 0 \n",
    "    return dicA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "086e24ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sn': 0, 'sa': 0, 'sv': 0, 'nn': 0, 'na': 0, 'nv': 0, 'an': 0, 'aa': 0, 'av': 0, 'vn': 0, 'va': 0, 'vv': 0}\n"
     ]
    }
   ],
   "source": [
    "dicA = init_dicA(set_pos)\n",
    "print(dicA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8631ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dicB(set_words,set_pos):\n",
    "    dicB= dict()\n",
    "    for i in set_pos:\n",
    "        for j in set_words:\n",
    "            key = i+j \n",
    "            dicB[key] = 0 \n",
    "    return dicB\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb741713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ncầu lông': 0, 'nthể dục': 0, 'nđi': 0, 'ntập': 0, 'ntôi': 0, 'nchơi': 0, 'acầu lông': 0, 'athể dục': 0, 'ađi': 0, 'atập': 0, 'atôi': 0, 'achơi': 0, 'vcầu lông': 0, 'vthể dục': 0, 'vđi': 0, 'vtập': 0, 'vtôi': 0, 'vchơi': 0}\n"
     ]
    }
   ],
   "source": [
    "dicB = init_dicB(set_words,set_pos)\n",
    "print(dicB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07865d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S V N A \n",
    "def count_pos_A(list_pos, dicA):\n",
    "    tmp = 's'\n",
    "    for i in range(1, len(list_pos)):\n",
    "        key = tmp + list_pos[i]\n",
    "        if i == 1: \n",
    "            if key in dicA: \n",
    "                dicA[key] +=1\n",
    "            else:\n",
    "                dicA[key] = 1\n",
    "        else:\n",
    "\n",
    "            if key not in dicA: \n",
    "                dicA[key] = 0\n",
    "\n",
    "    for i in range(0,len(list_pos)):\n",
    "        for j in range(i,len(list_pos)):\n",
    "            key = list_pos[i]+list_pos[j]\n",
    "            if j == i+1:\n",
    "                if key in dicA: \n",
    "                    dicA[key] +=1\n",
    "                else:\n",
    "                    dicA[key] = 1 \n",
    "            else:\n",
    "                if key not in dicA: \n",
    "                    dicA[key] = 0\n",
    "\n",
    "    return dicA \n",
    "            \n",
    "\n",
    "# [['a', 'a', 'n', 'v', 'a', 'n', 'a'], ['a', 'a', 'n', 'v', 'v', 'a', 'n', 'n']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed4ea073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrixA(list_pos,dicA):\n",
    "    for e in list_pos:\n",
    "        count_pos_A(e,dicA )\n",
    "    return dicA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d35c9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sn': 0, 'sa': 0, 'sv': 2, 'nn': 0, 'na': 0, 'nv': 2, 'an': 0, 'aa': 0, 'av': 0, 'vn': 0, 'va': 2, 'vv': 1}\n"
     ]
    }
   ],
   "source": [
    "A = matrixA (list_pos,dicA)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "498b6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos_B(list_words,list_pos,dicB):\n",
    "    for (item1,item2) in zip(list_pos,list_words):\n",
    "        for (item3,item4 )in zip (item1,item2):\n",
    "            key = item3 + item4 \n",
    "            dicB[key]+=1 \n",
    "    return dicB \n",
    "    \n",
    "\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "900afd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ncầu lông': 0, 'nthể dục': 0, 'nđi': 0, 'ntập': 0, 'ntôi': 2, 'nchơi': 0, 'acầu lông': 1, 'athể dục': 1, 'ađi': 0, 'atập': 0, 'atôi': 0, 'achơi': 0, 'vcầu lông': 0, 'vthể dục': 0, 'vđi': 1, 'vtập': 1, 'vtôi': 0, 'vchơi': 1}\n"
     ]
    }
   ],
   "source": [
    "dicB = count_pos_B(list_words,list_pos,dicB)\n",
    "print(dicB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "277d848c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def smoothing():\n",
    "    for e in A.keys():\n",
    "        A[e]=  A[e] +1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47769bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sn': 1, 'sa': 1, 'sv': 3, 'nn': 1, 'na': 1, 'nv': 3, 'an': 1, 'aa': 1, 'av': 1, 'vn': 1, 'va': 3, 'vv': 2}\n"
     ]
    }
   ],
   "source": [
    "smoothing()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81c0e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSum():\n",
    "    newdict = B.copy()\n",
    "    for key, value in B.items() :\n",
    "        key = key[0]+'sum'\n",
    "        if key in newdict: \n",
    "            newdict[key] += value\n",
    "        else:\n",
    "            newdict[key] = value\n",
    "            \n",
    "    return newdict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ff76e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sn': 1, 'sa': 1, 'sv': 3, 'nn': 1, 'na': 1, 'nv': 3, 'an': 1, 'aa': 1, 'av': 1, 'vn': 1, 'va': 3, 'vv': 2, 'ssum': 5, 'nsum': 5, 'asum': 3, 'vsum': 6}\n"
     ]
    }
   ],
   "source": [
    "A  = calculateSum()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d917e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sn': 0.2, 'sa': 0.2, 'sv': 0.6, 'nn': 0.2, 'na': 0.2, 'nv': 0.6, 'an': 0.3333333333333333, 'aa': 0.3333333333333333, 'av': 0.3333333333333333, 'vn': 0.16666666666666666, 'va': 0.5, 'vv': 0.3333333333333333, 'ssum': 15.0, 'nsum': 15.0, 'asum': 15.0, 'vsum': 15.0}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dd9a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePro():\n",
    "#     newdict = A.copy()\n",
    "    for key in A:\n",
    "        key_sum = key[0]+'sum'\n",
    "        A[key] =  A[key]/A[key_sum]\n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52a37c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sn': 0.2, 'sa': 0.2, 'sv': 0.6, 'nn': 0.2, 'na': 0.2, 'nv': 0.6, 'an': 0.3333333333333333, 'aa': 0.3333333333333333, 'av': 0.3333333333333333, 'vn': 0.16666666666666666, 'va': 0.5, 'vv': 0.3333333333333333, 'ssum': 1.0, 'nsum': 1.0, 'asum': 1.0, 'vsum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "A = calculatePro()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0995999c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "init_dicB() missing 2 required positional arguments: 'set_words' and 'set_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DIENNG~1\\AppData\\Local\\Temp/ipykernel_12968/2204646147.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdicB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dicB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: init_dicB() missing 2 required positional arguments: 'set_words' and 'set_pos'"
     ]
    }
   ],
   "source": [
    "dicB = init_dicB()\n",
    "print(dicB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639edaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
